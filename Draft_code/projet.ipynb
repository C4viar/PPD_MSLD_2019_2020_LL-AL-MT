{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/C4viar/PPD_MSLD_2019_2020_LL-AL-MT/blob/master/Draft_code/projet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRR2FKEa2hqj",
        "colab_type": "text"
      },
      "source": [
        "# Question 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzDHUpGfZOdd",
        "colab_type": "text"
      },
      "source": [
        "1. Reproduire avec l’implémentation\n",
        "https://huggingface.co/transformers/model_doc/camembert.html\n",
        "les manipulations décrites dans https://github.com/pytorch/fairseq/tree/master/examples/camembert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBgE2pvxd3OB",
        "colab_type": "text"
      },
      "source": [
        "## CamemBERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE3vTsynouCQ",
        "colab_type": "text"
      },
      "source": [
        "Rappel : Tous les types de modèles : https://huggingface.co/models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrCGiAgBBuJW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install transformers;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmYfZW5vX4KO",
        "colab_type": "text"
      },
      "source": [
        "#### Fill mask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGx-9wrQZ41s",
        "colab_type": "code",
        "outputId": "1676d7bc-853d-4f41-b815-e722ab483e8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "source": [
        "from transformers import pipeline, CamembertModel, CamembertTokenizer, CamembertForMaskedLM, PreTrainedModel\n",
        "\n",
        "\n",
        "sentence_fillmask = \"\"\"Le camembert c'est <mask>\"\"\".lstrip()\n",
        "model_fillmask_str = \"camembert-base\"\n",
        "model_fillmask = CamembertForMaskedLM.from_pretrained(model_fillmask_str )\n",
        "tokenizer_fillmask = CamembertTokenizer.from_pretrained(model_fillmask_str )\n",
        "\n",
        "\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model = model_fillmask,\n",
        "    tokenizer = tokenizer_fillmask \n",
        "    )\n",
        "\n",
        "fill_mask(sentence_fillmask)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.2072705626487732,\n",
              "  'sequence': \"<s> Le camembert c'est quoi</s>\",\n",
              "  'token': 484},\n",
              " {'score': 0.08772061765193939,\n",
              "  'sequence': \"<s> Le camembert c'est bon</s>\",\n",
              "  'token': 212},\n",
              " {'score': 0.05717896297574043,\n",
              "  'sequence': \"<s> Le camembert c'est délicieux</s>\",\n",
              "  'token': 7200},\n",
              " {'score': 0.05560891702771187,\n",
              "  'sequence': \"<s> Le camembert c'est...</s>\",\n",
              "  'token': 186},\n",
              " {'score': 0.05323265120387077,\n",
              "  'sequence': \"<s> Le camembert c'est :</s>\",\n",
              "  'token': 43}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJlubl3jBE6m",
        "colab_type": "text"
      },
      "source": [
        "#### Sentiment analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imQ9nbvhxF8u",
        "colab_type": "text"
      },
      "source": [
        "Indisponible avec Camembert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "It4Id0s-BD-v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import CamembertForTokenClassification\n",
        "\n",
        "sentence_sentiment = \"Le camembert c'est bon\"\n",
        "\n",
        "model_sentiment_str = \"\"\n",
        "model_sentiment = CamembertForSequenceClassification.from_pretrained(model_sentiment_str)\n",
        "tokenizer_sentiment = CamembertTokenizer.from_pretrained(model_sentiment_str )\n",
        "\n",
        "sentiment_analysis = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model = model_sentiment,\n",
        "    tokenizer = tokenizer_sentiment\n",
        "    )\n",
        "\n",
        "sentiment_analysis(sentence_sentiment)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7V55psHqNz_2",
        "colab_type": "text"
      },
      "source": [
        "#### NER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_XxrSLmyAAz",
        "colab_type": "text"
      },
      "source": [
        "Indisponible avec Camembert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kf0Jl_b-N6EF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import CamembertForTokenClassification\n",
        "\n",
        "sentence_ner = \"Amar habite à Paris\"\n",
        "\n",
        "ner_model_str = ''\n",
        "ner_model = CamembertForTokenClassification.from_pretrained(ner_model_str)\n",
        "tokenizer_ner= CamembertTokenizer.from_pretrained(ner_model_str)\n",
        "\n",
        "ner = pipeline(\n",
        "    \"ner\",\n",
        "    model = ner_model,\n",
        "    tokenizer = tokenizer_ner\n",
        "    )\n",
        "\n",
        "ner(sentence_net)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKjwmr34tZeV",
        "colab_type": "text"
      },
      "source": [
        "#### Question answering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbojeRDMtWxb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import CamembertForTokenClassification\n",
        "\n",
        "quest_model_str = ''\n",
        "quest_model = XXXXXXX.from_pretrained(quest_model_str)  # XXXXXXX à remplacer / ! \\\n",
        "tokenizer_quest= CamembertTokenizer.from_pretrained(quest_model_str)\n",
        "\n",
        "quest = pipeline(\n",
        "    \"question_answering\",\n",
        "    model = quest_model,\n",
        "    tokenizer = tokenizer_quest\n",
        "    )\n",
        "\n",
        "quest(\n",
        "    'question' : \"Où habite Amar ? \",\n",
        "    'context' : \"Amar habite à Paris.\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQ_xS0dptpqz",
        "colab_type": "text"
      },
      "source": [
        "#### Translation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOUfCBs2toJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWXE8HxOGkDH",
        "colab_type": "text"
      },
      "source": [
        "#### Feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBxn6P_KGKkS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_extraction = pipeline(\n",
        "    \"feature-extraction\",\n",
        "    model = model_camembert,\n",
        "    tokenizer = tokenizer_camembert\n",
        "    )\n",
        "\n",
        "feature_extraction(sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyOPz8ueX8XB",
        "colab_type": "text"
      },
      "source": [
        "#### Extract All-layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3BMWHCdVUy6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_ids = torch.tensor(tokenizer.encode(sentence, add_special_tokens=True)).unsqueeze(0)\n",
        "labels = torch.tensor([1] * input_ids.size(1)).unsqueeze(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntWY-sYzfjDX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "outputs_camembert = model(input_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lru2I3GmXoaA",
        "colab_type": "code",
        "outputId": "76ebf8bb-e6e7-402f-cc2e-8345730b41cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "source": [
        "outputs_cammebert"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[ 20.6235,  -4.2011,   6.8420,  ...,  -5.7573,  -3.1165,   1.1391],\n",
              "          [  2.6217,  -5.2430,  17.5290,  ..., -14.2041,  -2.4229,   0.8586],\n",
              "          [ -0.1691,  -9.8924,   1.8213,  ..., -22.9433, -11.8068,  -1.9409],\n",
              "          ...,\n",
              "          [ -0.6569, -10.2129,  -0.5032,  ...,  -7.7102, -10.5380,   1.3649],\n",
              "          [ -1.6542,  -4.3773,   2.8810,  ...,  -7.5383,  -3.0471,  -3.1600],\n",
              "          [  6.3686,  -5.1610,  23.3708,  ...,  -7.9306,  -5.1287,   1.9971]]],\n",
              "        grad_fn=<AddBackward0>),)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhprqb6qd6jN",
        "colab_type": "text"
      },
      "source": [
        "## FlauBERT\n",
        "### Ne fonctionne pas pour le moment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihcDvmk8Vo1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import FlaubertModel, AutoTokenizer, AutoModelWithLMHead\n",
        "\n",
        "flaubert = \"flaubert-large-cased\"\n",
        "\n",
        "tokenizer_flaubert = AutoTokenizer.from_pretrained(flaubert)\n",
        "model_flaubert = AutoModelWithLMHead.from_pretrained(flaubert)\n",
        "\n",
        "\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model = flaubert,\n",
        "    tokenizer = flaubert\n",
        ")\n",
        "\n",
        "fill_mask(sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jt446DKKJhn",
        "colab_type": "text"
      },
      "source": [
        "### Flaubert : essai 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5MrIfeKhFO3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import torch\n",
        "from transformers import FlaubertModel, FlaubertTokenizer\n",
        "\n",
        "# Choose among ['flaubert-small-cased', 'flaubert-base-uncased', 'flaubert-base-cased', 'flaubert-large-cased']\n",
        "modelname = 'flaubert-base-cased' \n",
        "\n",
        "# Load pretrained model and tokenizer\n",
        "flaubert, log = FlaubertModel.from_pretrained(modelname, output_loading_info=True)\n",
        "flaubert_tokenizer = FlaubertTokenizer.from_pretrained(modelname, do_lowercase=False)\n",
        "# do_lowercase=False if using cased models, True if using uncased ones\n",
        "\n",
        "sentence = \"Le camembert c'est <mask>.\"\n",
        "token_ids = torch.tensor([flaubert_tokenizer.encode(sentence)])\n",
        "\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model = flaubert,\n",
        "    tokenizer = flaubert_tokenizer\n",
        ")\n",
        "\n",
        "fill_mask(sentence)\n",
        "\n",
        "#last_layer = flaubert(token_ids)[0]\n",
        "#print(last_layer.shape)\n",
        "# torch.Size([1, 8, 768])  -> (batch size x number of tokens x embedding dimension)\n",
        "\n",
        "# The BERT [CLS] token correspond to the first hidden state of the last layer\n",
        "#cls_embedding = last_layer[:, 0, :]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vdeiAQpSQgM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}