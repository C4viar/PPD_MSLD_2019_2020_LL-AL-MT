{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/C4viar/PPD_MSLD_2019_2020_LL-AL-MT/blob/master/Draft_code/projet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4ZqJzN4vZZK",
        "colab_type": "text"
      },
      "source": [
        "# Réplication de l'exemple GIT\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvF_g7MU5sQw",
        "colab_type": "text"
      },
      "source": [
        "## Installation des packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Edvb4dSeoD0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEsO0FTao_2H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVyTVavZ5zs0",
        "colab_type": "text"
      },
      "source": [
        "## Imports "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zApjyvXoNcM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os \n",
        "import torch\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UiCxfPkpJIr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "c9872ac2-3192-4aab-bb76-7936336859ec"
      },
      "source": [
        "\n",
        "camembert = torch.hub.load('pytorch/fairseq', 'camembert.v0');\n",
        "camembert.eval();  # disable dropout (or leave in train mode to finetune)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_fairseq_master\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aHLZ-bFpL_Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download camembert model\n",
        "! wget 'https://dl.fbaipublicfiles.com/fairseq/models/camembert.v0.tar.gz' ; \n",
        "! tar -xzvf camembert.v0.tar.gz ;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wz11Yexap_Sy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the model in fairseq\n",
        "from fairseq.models.roberta import CamembertModel\n",
        "camembert = CamembertModel.from_pretrained('/content/camembert.v0');\n",
        "camembert.eval();  # disable dropout (or leave in train mode to finetune)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QKADBnuqdgK",
        "colab_type": "code",
        "outputId": "6471f7aa-9a99-49d2-e302-04db0847b918",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "masked_line = 'Le camembert est <mask> :)'\n",
        "camembert.fill_mask(masked_line, topk=3)\n"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Le camembert est délicieux :)', 0.4909120500087738, ' délicieux'),\n",
              " ('Le camembert est excellent :)', 0.10556947439908981, ' excellent'),\n",
              " ('Le camembert est succulent :)', 0.034533340483903885, ' succulent')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aritoVZ4RzAg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract the last layer's features\n",
        "line = \"J'aime le camembert !\"\n",
        "tokens = camembert.encode(line)\n",
        "last_layer_features = camembert.extract_features(tokens)\n",
        "assert last_layer_features.size() == torch.Size([1, 10, 768])\n",
        "\n",
        "# Extract all layer's features (layer 0 is the embedding layer)\n",
        "all_layers = camembert.extract_features(tokens, return_all_hiddens=True)\n",
        "assert len(all_layers) == 13\n",
        "assert torch.all(all_layers[-1] == last_layer_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRR2FKEa2hqj",
        "colab_type": "text"
      },
      "source": [
        "# Question 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzDHUpGfZOdd",
        "colab_type": "text"
      },
      "source": [
        "1. Reproduire avec l’implémentation\n",
        "https://huggingface.co/transformers/model_doc/camembert.html\n",
        "les manipulations décrites dans https://github.com/pytorch/fairseq/tree/master/examples/camembert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBgE2pvxd3OB",
        "colab_type": "text"
      },
      "source": [
        "## CamemBERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE3vTsynouCQ",
        "colab_type": "text"
      },
      "source": [
        "Rappel : Tous les types de modèles : https://huggingface.co/models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrCGiAgBBuJW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install transformers;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmYfZW5vX4KO",
        "colab_type": "text"
      },
      "source": [
        "#### Fill mask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGx-9wrQZ41s",
        "colab_type": "code",
        "outputId": "87b59487-0059-4874-dbbc-83c2e7732040",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "source": [
        "from transformers import pipeline, CamembertModel, CamembertTokenizer\n",
        "\n",
        "\n",
        "sentence = \"\"\"Le camembert c'est <mask>\"\"\".lstrip()\n",
        "\n",
        "camembert = \"camembert-base\"\n",
        "\n",
        "model_camembert = CamembertModel.from_pretrained(camembert)\n",
        "\n",
        "tokenizer_camembert = CamembertTokenizer.from_pretrained(camembert)\n",
        "\n",
        "\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model = model_camembert,\n",
        "    tokenizer = tokenizer_camembert\n",
        "    )\n",
        "\n",
        "fill_mask(sentence)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.5193097591400146,\n",
              "  'sequence': \"<s> Le camembert c'est jours</s>\",\n",
              "  'token': 274},\n",
              " {'score': 0.003460721345618367,\n",
              "  'sequence': \"<s> Le camembert c'esten</s>\",\n",
              "  'token': 90},\n",
              " {'score': 0.001355467364192009,\n",
              "  'sequence': \"<s> Le camembert c'est femme</s>\",\n",
              "  'token': 270},\n",
              " {'score': 0.0010079997591674328,\n",
              "  'sequence': \"<s> Le camembert c'estp</s>\",\n",
              "  'token': 286},\n",
              " {'score': 0.0009088139049708843,\n",
              "  'sequence': \"<s> Le camembert c'est T</s>\",\n",
              "  'token': 309}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyOPz8ueX8XB",
        "colab_type": "text"
      },
      "source": [
        "#### Extract All-layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3BMWHCdVUy6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_ids = torch.tensor(tokenizer.encode(sentence, add_special_tokens=True)).unsqueeze(0)\n",
        "labels = torch.tensor([1] * input_ids.size(1)).unsqueeze(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntWY-sYzfjDX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "outputs_camembert = model(input_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lru2I3GmXoaA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "outputId": "76ebf8bb-e6e7-402f-cc2e-8345730b41cf"
      },
      "source": [
        "outputs_cammebert"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[ 20.6235,  -4.2011,   6.8420,  ...,  -5.7573,  -3.1165,   1.1391],\n",
              "          [  2.6217,  -5.2430,  17.5290,  ..., -14.2041,  -2.4229,   0.8586],\n",
              "          [ -0.1691,  -9.8924,   1.8213,  ..., -22.9433, -11.8068,  -1.9409],\n",
              "          ...,\n",
              "          [ -0.6569, -10.2129,  -0.5032,  ...,  -7.7102, -10.5380,   1.3649],\n",
              "          [ -1.6542,  -4.3773,   2.8810,  ...,  -7.5383,  -3.0471,  -3.1600],\n",
              "          [  6.3686,  -5.1610,  23.3708,  ...,  -7.9306,  -5.1287,   1.9971]]],\n",
              "        grad_fn=<AddBackward0>),)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhprqb6qd6jN",
        "colab_type": "text"
      },
      "source": [
        "## FlauBERT\n",
        "### Ne fonctionne pas pour le moment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihcDvmk8Vo1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import FlaubertModel, AutoTokenizer, AutoModelWithLMHead\n",
        "\n",
        "flaubert = \"flaubert-large-cased\"\n",
        "\n",
        "tokenizer_flaubert = AutoTokenizer.from_pretrained(flaubert)\n",
        "model_flaubert = AutoModelWithLMHead.from_pretrained(flaubert)\n",
        "\n",
        "\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model = flaubert,\n",
        "    tokenizer = flaubert\n",
        ")\n",
        "\n",
        "fill_mask(sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jt446DKKJhn",
        "colab_type": "text"
      },
      "source": [
        "### Flaubert : essai 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5MrIfeKhFO3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import torch\n",
        "from transformers import FlaubertModel, FlaubertTokenizer\n",
        "\n",
        "# Choose among ['flaubert-small-cased', 'flaubert-base-uncased', 'flaubert-base-cased', 'flaubert-large-cased']\n",
        "modelname = 'flaubert-base-cased' \n",
        "\n",
        "# Load pretrained model and tokenizer\n",
        "flaubert, log = FlaubertModel.from_pretrained(modelname, output_loading_info=True)\n",
        "flaubert_tokenizer = FlaubertTokenizer.from_pretrained(modelname, do_lowercase=False)\n",
        "# do_lowercase=False if using cased models, True if using uncased ones\n",
        "\n",
        "sentence = \"Le camembert c'est <mask>.\"\n",
        "token_ids = torch.tensor([flaubert_tokenizer.encode(sentence)])\n",
        "\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model = flaubert,\n",
        "    tokenizer = flaubert_tokenizer\n",
        ")\n",
        "\n",
        "fill_mask(sentence)\n",
        "\n",
        "#last_layer = flaubert(token_ids)[0]\n",
        "#print(last_layer.shape)\n",
        "# torch.Size([1, 8, 768])  -> (batch size x number of tokens x embedding dimension)\n",
        "\n",
        "# The BERT [CLS] token correspond to the first hidden state of the last layer\n",
        "#cls_embedding = last_layer[:, 0, :]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vdeiAQpSQgM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}